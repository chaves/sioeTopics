{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ftfy import fix_text # Fix any unicode problem\n",
    "import spacy              # Prepare the data\n",
    "import csv\n",
    "import pickle\n",
    "import glob, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_SOURCES = 'sources_data/'\n",
    "GB_US_synonyms_file = DATA_SOURCES + 'gb-us-synonyms.txt'\n",
    "expands_file = DATA_SOURCES + 'expands.txt'\n",
    "DATA_CLEAN = 'clean_data/'\n",
    "years_available = list(range(2008,2018))\n",
    "\n",
    "# noisy_pos_tags = ['PROP'] # noisy tags\n",
    "noisy_pos_tags = [\"PROP\",\"DET\",\"PART\",\"CCONJ\",\"ADP\",\"PRON\",\"VERB\",\"ADJ\"]\n",
    "# DET  = definite or indefinite article\n",
    "# ADP  = conjunction, subordinating or preposition\n",
    "# PART = adverb, particle\n",
    "# ADP  = postposition => in\n",
    "# PRON = pronoun, personal => I\n",
    "# VERB, ADJ\n",
    "min_token_length = 2 # minimum token length to remove \n",
    "common_token = 20 # most characteristic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first step to use spaCy is to constructs a \n",
    "# language processing pipeline, here we're loading\n",
    "# the pre-trained english model\n",
    "from spacy.en import English\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_noise(token):\n",
    "    '''\n",
    "    standard way to validate spacy tokens\n",
    "    This method validate all the passed tokens and set true false on it\n",
    "    '''\n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True \n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "    elif token.is_digit == True:\n",
    "        is_noise = True\n",
    "    elif token.is_punct == True:\n",
    "        is_noise = True\n",
    "    elif token.is_space == True:\n",
    "        is_noise = True\n",
    "    elif len(token.string) <= min_token_length:\n",
    "        is_noise = True\n",
    "    return is_noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stop_words = ['effect', 'find', 'iii', 'e.g.', 'i.e.', 'al.', 'evidence', 'article', 'paper', \\\n",
    "                 'result', 'results', 'author', 'authors', 'v.s.']\n",
    "for stop in my_stop_words:\n",
    "    nlp.vocab[stop].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_list(file):\n",
    "    with open(file, mode='r') as file:\n",
    "        terms = csv.reader(file)\n",
    "        return {rows[0]:rows[1] for rows in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gb_to_us(words):\n",
    "    '''\n",
    "    Replace British English with American English\n",
    "    Important since it concerns an international conference\n",
    "    e.g. both organisation and organization terms are used regularly\n",
    "    source : https://github.com/7digital/synonym-list/\n",
    "    '''\n",
    "    gb_us = get_list(GB_US_synonyms_file)\n",
    "    for key in gb_us:\n",
    "        words = words.replace(key, gb_us[key])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_terms(words):\n",
    "    '''\n",
    "    Expand usual terms used in the field\n",
    "    '''\n",
    "    expands = get_list(expands_file)\n",
    "    for key in expands:\n",
    "        words = words.replace(key, expands[key])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_specific_stop(words):\n",
    "    punct = ['%', ',', '/', '(', ')', '.'] # frequent punctuation terms inside strings or digits\n",
    "    for p in punct:\n",
    "        words = words.replace(p, ' ')\n",
    "        words = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = 'firm nonmarket strategy rent merger acquisition 1990 35% investor'\n",
    "# out = remove_specific_stop(test)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, article = [], []\n",
    "texts_txt = ''\n",
    "for year in years_available:\n",
    "    file_list = glob.glob(\"sources_data/{}/*.txt\".format(year))\n",
    "    for f in file_list:\n",
    "        words = open(f).read()\n",
    "        words = fix_text(words) # Fix any unicode problem\n",
    "        words = words.replace('\\n', ' ').replace('\\r', '') # remove line breaks\n",
    "        words = remove_specific_stop(words)\n",
    "        # words = expand_terms(words)\n",
    "        words = gb_to_us(words)\n",
    "        if(len(words.split()) >= 30): # Only abstracts with at least 30 words\n",
    "            nlp_words = nlp(words)\n",
    "            for word in nlp_words:\n",
    "                if not is_noise(word):\n",
    "                    article.append(word.lemma_)\n",
    "            texts.append(article)\n",
    "            texts_txt = texts_txt + ' '.join(article) + '\\n'\n",
    "            article = []\n",
    "    \n",
    "    with open(\"{}{}.pickle\".format(DATA_CLEAN, year), \"wb\") as fp:\n",
    "        pickle.dump(texts, fp)\n",
    "    \n",
    "    with open(\"{}{}.txt\".format(DATA_CLEAN, year), \"w\") as fp:\n",
    "        fp.write(texts_txt)\n",
    "    \n",
    "    texts_txt = ''\n",
    "    texts = []\n",
    "    print('Year ' + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
